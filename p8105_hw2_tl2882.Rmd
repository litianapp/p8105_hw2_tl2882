---
title: "p8105_hw2_tl2882"
author: "Tian Li"
date: 2018-10-03
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

# Problem 1

```{r read_and_tidy_data}
data1 = read.csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names() %>% 
  select(line, station_name, station_latitude, station_longitude, route1:route11,
         entry, vending, entrance_type, ada) %>% 
  mutate(entry = recode(entry, "YES" = TRUE, "NO" = FALSE))
```

The variables the dataset contains are names of line, names of station, latitudes of station, longitudes of station, train of route1 to route11, entry allowed or not, with vending or not, type of entrance, ADA compliant or not.

Data cleaning steps so far include changing original variable names into reasonable names, selecting needed variables (columns), and converting the entry variable from character (YES vs NO) to a logical variable (TRUE vs FALSE).

The dimension of resulting dataset is (```r  dim(data1) ```) (the first number is the number of the rows, the second number is the number of the columns)

These data are not tidy because there are columns named route1 to route11.

## Problem 1.1

```{r Problem1_1}
data1_distinct = distinct(data1, line, station_name, .keep_all = TRUE)
nrow(data1_distinct)
```

There are 465 distinct stations.

## Problem 1.2

```{r Problem1_2}
nrow(filter(data1_distinct, ada == TRUE))
```

84 distinct stations are ADA compliant.

## Problem 1.3

```{r Problem1_3}
data1_withoutvending = filter(data1, vending == "NO")
sum(data1_withoutvending$entry)/length(data1_withoutvending$entry)
```

37.70492% of station entrances / exits without vending allow entrance.

## Reformat data
```{r Reformat_data}
data1_A_distinct = gather(data1, key = route, value = train, route1:route11) %>% 
  filter(train == "A") %>% 
  distinct(line, station_name, .keep_all = TRUE)
nrow(data1_A_distinct)
nrow(filter(data1_A_distinct, ada == TRUE))
```

60 distinct stations serve the A train.

Of the stations that serve the A train, 17 stations are ADA compliant.

# Problem 2

## Read and tidy the Mr. Trash Wheel sheet:

```{r read_and_tidy_data2}
library(readxl)
data2 = read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx",
                 range = cell_cols("A:N"))  %>% 
  janitor::clean_names() %>%
  filter(!is.na(dumpster)) %>% 
  mutate(sports_balls = as.integer(round(sports_balls)))
```

## Read and tidy precipitation data for 2016 and 2017:

```{r read_and_tidy_precipitation_data}
prec_2016 = read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx",
                   sheet = "2016 Precipitation", range = "A2:B14")  %>% 
  mutate(year = "2016")

prec_2017 = read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx",
                       sheet = "2017 Precipitation", range = "A2:B14")  %>% 
  mutate(year = "2017")

data_prec = bind_rows(prec_2016, prec_2017) %>% 
  janitor::clean_names() %>%
  mutate(month = month.name[month])
```

In Mr.Trash Wheel dataset, the number of observation is ```r nrow(data2)```. The key variable I choose is "Dumpster".

In percipitation dataset, the number of observation is ```r nrow(data_prec)```. The key variable I choose is "Total".

For available data, the total precipitation in 2017 was ```r sum(filter(data_prec, year == "2017")$total)```.

The median number of sports balls in a dumpster in 2016 was```r median(filter(data2, year == "2016")$sports_balls)```.

# Problem 3

## Install github dataset

```{r install_github_dataset}
# install.packages("devtools")
devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)
```

## Read and tidy brfss_smart2010

```{r Read_and_tidy_brfss_smart2010}
brfss = brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  select(-class, -topic, -question, -sample_size, -(confidence_limit_low:geo_location)) %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>%
  mutate(excel_or_verygood = excellent + very_good)
```

## Problem 3.1

```{r Problem_3_1}
nrow(distinct(brfss, locationdesc))
nrow(distinct(brfss, locationabbr))
arrange(count(brfss, brfss$locationabbr), desc(n))
```

404 unique locations are included in the dataset.

All 50 states and Washington District of Columbia are represented.

New Jersey state is observed the most (146 times).

## Problem 3.2

```{r Problem_3_2}
brfss_2002 = filter(brfss, year == "2002")
median(brfss_2002$excellent, na.rm = TRUE)
```

In 2002, the median of the “Excellent” response value is 23.6.

## Problem 3.3

```{r Problem_3_3}
ggplot(brfss_2002, aes(x = excellent)) +
  geom_histogram() + 
  labs(
    title = "Excellent response values in the year 2002", 
    x = "Excellent response values (%)", 
    caption = "Data from p8105.datasets"
  )
```

This is the histogram of “Excellent” response values in the year 2002.

## Problem 3.4

```{r Problem_3_4}
filter(brfss, locationdesc == "NY - New York County" | locationdesc == "NY - Queens County") %>% 
  ggplot(aes(x = year, y = excellent, color = locationdesc)) + 
    geom_point() + 
    labs(
      title = "Excellent Proportion in NewYork & Queens from 2002 to 2010",
      y = "proportion of Excellent response (%)", 
      caption = "Data from p8105.datasets"
    ) + 
    viridis::scale_color_viridis(name = "Location", discrete = TRUE)
```

This is the scatterplot showing the proportion of “Excellent” response values in New York County and Queens County (both in NY State) in each year from 2002 to 2010.